---
title: "Machine Learning with Titanic"
author: "Leon Shpaner"
date: "2020-07-06"
output:
  pdf_document: default
  html_document: default
image:
  caption: ''
  focal_point: ''
  preview_only: yes
tags:
- R
- RMarkdown
- k-nearest neighbors
- KNN
- Machine Learning
- Titanic Survival
- Train
- Test
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>This project is based on a dataset of passengers from the RMS Titanic, a famous luxury cruise liner that sank in 1912. (For more details on the Titanic, see <a href="https://en.wikipedia.org/wiki/RMS_Titanic" class="uri">https://en.wikipedia.org/wiki/RMS_Titanic</a>.) The dataset comes from Kaggle (<a href="https://www.kaggle.com/c/titanic" class="uri">https://www.kaggle.com/c/titanic</a>). Kaggle hosts data science competitions and is a great resource for practicing predictive analytics skills. Provided herein are test and train datasets for the titanic scenario.</p>
<p><em>Note.</em> Models are built with train and tested with test.</p>
<p>Working with the train.csv dataset the following steps were followed:</p>
<ul>
<li>Imported the titanic.csv into a data frame in R</li>
<li>Generated a series of descriptive statistics</li>
<li>Determined if there were any variables with missing observations</li>
<li>Generated a series of visualizations to better understand the sample</li>
</ul>
<p>The ultimate goal of the project was to build models to determine which passengers were most likely to have survived the sinking of the Titanic. In this first part of the project, we will focus on just describing the data by providing some insight into who lived and died when the Titanic sank (variable Survived in the sample). Variables are supporting insights with descriptive statistics and visualizations generated in R.</p>
<ol style="list-style-type: decimal">
<li>Install the readr package, load the library, and load titanic.csv into the data frame named boat</li>
</ol>
<pre class="r numberLines lineAnchors numberLines"><code>#install.packages(&quot;readr&quot;)
library(readr)
boat &lt;- read.csv(&quot;train.csv&quot;)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Return a vector (or matrix) in the same dimension as data using the boat data frame and class function. Use the summary function to quickly summarize the sample</li>
</ol>
<pre class="r numberLines lineAnchors"><code>sapply(boat,class)
## PassengerId    Survived      Pclass        Name         Sex         Age 
##   &quot;integer&quot;   &quot;integer&quot;   &quot;integer&quot; &quot;character&quot; &quot;character&quot;   &quot;numeric&quot; 
##       SibSp       Parch      Ticket        Fare       Cabin    Embarked 
##   &quot;integer&quot;   &quot;integer&quot; &quot;character&quot;   &quot;numeric&quot; &quot;character&quot; &quot;character&quot;

summary(boat)
##   PassengerId       Survived          Pclass          Name          
##  Min.   :  1.0   Min.   :0.0000   Min.   :1.000   Length:891        
##  1st Qu.:223.5   1st Qu.:0.0000   1st Qu.:2.000   Class :character  
##  Median :446.0   Median :0.0000   Median :3.000   Mode  :character  
##  Mean   :446.0   Mean   :0.3838   Mean   :2.309                     
##  3rd Qu.:668.5   3rd Qu.:1.0000   3rd Qu.:3.000                     
##  Max.   :891.0   Max.   :1.0000   Max.   :3.000                     
##                                                                     
##      Sex                 Age            SibSp           Parch       
##  Length:891         Min.   : 0.42   Min.   :0.000   Min.   :0.0000  
##  Class :character   1st Qu.:20.12   1st Qu.:0.000   1st Qu.:0.0000  
##  Mode  :character   Median :28.00   Median :0.000   Median :0.0000  
##                     Mean   :29.70   Mean   :0.523   Mean   :0.3816  
##                     3rd Qu.:38.00   3rd Qu.:1.000   3rd Qu.:0.0000  
##                     Max.   :80.00   Max.   :8.000   Max.   :6.0000  
##                     NA&#39;s   :177                                     
##     Ticket               Fare           Cabin             Embarked        
##  Length:891         Min.   :  0.00   Length:891         Length:891        
##  Class :character   1st Qu.:  7.91   Class :character   Class :character  
##  Mode  :character   Median : 14.45   Mode  :character   Mode  :character  
##                     Mean   : 32.20                                        
##                     3rd Qu.: 31.00                                        
##                     Max.   :512.33                                        
## </code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Using relevant descriptive statistics, we can look at:</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>the average fare that the passengers paid:</li>
</ol>
<pre class="r numberLines lineAnchors"><code>mean(boat$Fare)
## [1] 32.20421</code></pre>
<ol start="2" style="list-style-type: lower-alpha">
<li>the average age of passengers on the Titanic while removing all missing (NA) values:</li>
</ol>
<pre class="r numberLines lineAnchors"><code>mean(as.numeric(boat$Age),na.rm=TRUE)
## [1] 29.69912</code></pre>
<ol start="3" style="list-style-type: lower-alpha">
<li>similarly, we can get the standard deviation as follows:</li>
</ol>
<pre class="r numberLines lineAnchors"><code>sd(as.numeric(boat$Age),na.rm=TRUE)
## [1] 14.5265</code></pre>
<ol start="4" style="list-style-type: lower-alpha">
<li>and the average (as a percentage) of those who survived:</li>
</ol>
<pre class="r numberLines lineAnchors"><code>mean(boat$Survived)
## [1] 0.3838384</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>To get a graphical comparison of who survived vs. who did not, we can see this in the following bar chart:</li>
</ol>
<pre class="r numberLines lineAnchors"><code>counts &lt;- table(boat$Survived)
par(las=2) # make label text perpendicular to axis
par(mar=c(5,8,4,2)) # increase y-axis margin.
barplot(counts, main=&quot;Survived vs. Not Survived&quot;, horiz = TRUE,
        names.arg=c(&quot;0 = Not Survived&quot;, &quot;1 = Survived&quot;),
        col=c(&quot;brown2&quot;,
        &quot;cornflowerblue&quot;),
        xlim=c(0,600),space=c(0,0)) </code></pre>
<p><img src="/projects/post/Titanic_Machine_Learning/index_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<ol start="5" style="list-style-type: decimal">
<li>Now we will generate a histogram to show the age distribution of passengers on the titanic.</li>
</ol>
<pre class="r numberLines lineAnchors"><code>h=hist(boat$Age,xlab=&quot;Age&quot;, ylab=&quot;Frequency&quot;, 
       main=&quot;Passenger Age Distribution&quot;, col=&quot;lightblue&quot;)</code></pre>
<p><img src="/projects/post/Titanic_Machine_Learning/index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<ol start="6" style="list-style-type: decimal">
<li>To further examine the data, we can also look at the age of Titanic Passengers by Gender:</li>
</ol>
<pre class="r numberLines lineAnchors"><code>boxplot(boat$Age~boat$Sex,xlab=&quot;Gender&quot;, ylab=&quot;Age&quot;, 
        main=&quot;Boxplot - Age of Titanic Passenger by Gender&quot;)</code></pre>
<p><img src="/projects/post/Titanic_Machine_Learning/index_files/figure-html/boxplot-1.png" width="672" /></p>
<p>Based on this basic model, we can see that 38% of the passengers survived based on the average we calculated in #3. Now, our goal is to dive deeper and select variables that we think will influence whether passengers survived, and then use k-nearest neighbors (KNN) to build classification models that will predict who survived the Titanic.</p>
<p>To accomplish this, we will load two additional libraries: class, and caTools. According to the documentation, class is a package that contains “various functions for classification, including k-nearest neighbour, Learning Vector Quantization and Self-Organizing Maps” (<a href="https://www.rdocumentation.org/packages/class/versions/7.3-17" class="uri">https://www.rdocumentation.org/packages/class/versions/7.3-17</a>). CaTools “contains several basic utility functions including: moving (rolling, running) window statistic functions, read/write for GIF and ENVI binary files, fast calculation of AUC, LogitBoost classifier, base64 encoder/decoder, round-off-error-free sum and cumsum, etc.” (<a href="https://www.rdocumentation.org/packages/caTools/versions/1.17.1" class="uri">https://www.rdocumentation.org/packages/caTools/versions/1.17.1</a>)</p>
<pre class="r numberLines lineAnchors"><code>library(class)
library(caTools)</code></pre>
<p>In order to classify these variables, we will use KNN as a statistical estimation/ pattern recognition tool. In a nutshell, the algorithm will classify new variables based on existing variables’ current classification.</p>
<p>Next, we want to see if there exists a relationship between the selected variables, but not all of the variables are quantitative, and as such, we must run a logistic regression, as follows.</p>
<pre class="r numberLines lineAnchors"><code>library(tree)
boat$AgeAVG&lt;-boat$Age
boat$Survived&lt;-as.factor(boat$Survived)
boatLR&lt;-glm(Survived~Sex+AgeAVG+SibSp+Parch,family=binomial(),data=boat)
#remove any variables from boat that you won’t use for your classification
#the following code uses Survived, Sex, Age, SibSp, and Parch 
# You can use your choice of variables, or fewer variables if you wish
boat&lt;-boat[,c(2,5,6,7,8)]
sum(is.na(boat$Age))
## [1] 177
boat&lt;-within(boat,Age[is.na(Age)]&lt;-mean(Age,na.rm=TRUE))
boat$Sex[boat$Sex==&quot;male&quot;]&lt;-1
boat$Sex[boat$Sex==&quot;female&quot;]&lt;-0
set.seed(123) 

sample&lt;-sample.split(boat$Sex, SplitRatio = .80)
train&lt;-subset(boat, sample == TRUE)
test&lt;-subset(boat, sample == FALSE)
knn1&lt;-knn(train[-1],test[-1],train$Sex, k=1)

train&lt;-subset(boat, sample == TRUE)
test&lt;-subset(boat, sample == FALSE)
knn1&lt;-knn(train[-1],test[-1],train$Age, k=1)

train&lt;-subset(boat, sample == TRUE)
test&lt;-subset(boat, sample == FALSE)
knn1&lt;-knn(train[-1],test[-1],train$Survived, k=1)

train&lt;-subset(boat, sample == TRUE)
test&lt;-subset(boat, sample == FALSE)
knn1&lt;-knn(train[-1],test[-1],train$SibSp, k=1)

train&lt;-subset(boat, sample == TRUE)
test&lt;-subset(boat, sample == FALSE)
knn1&lt;-knn(train[-1],test[-1],train$Parch, k=1)

CF&lt;-table(knn1,test$Age)

Precision&lt;-CF[2,2]/(CF[2,1]+CF[2,2])
Precision
## [1] 0.5

train&lt;-subset(boat, sample == TRUE)
test&lt;-subset(boat, sample == FALSE)
TrainTree&lt;-tree(Survived ~ Sex+Age+SibSp, data=train)

plot(TrainTree)
text(TrainTree, cex=.9)</code></pre>
<p><img src="/projects/post/Titanic_Machine_Learning/index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r numberLines lineAnchors"><code>summary(TrainTree)
## 
## Classification tree:
## tree(formula = Survived ~ Sex + Age + SibSp, data = train)
## Number of terminal nodes:  5 
## Residual mean deviance:  0.9354 = 662.3 / 708 
## Misclassification error rate: 0.1781 = 127 / 713
TrainPrune&lt;-prune.tree(TrainTree,best = 3,newdata=test,method = &quot;misclass&quot;)
plot(TrainPrune)
text(TrainPrune, cex=.9)</code></pre>
<p><img src="/projects/post/Titanic_Machine_Learning/index_files/figure-html/unnamed-chunk-9-2.png" width="672" /></p>
<pre class="r numberLines lineAnchors"><code>summary(TrainPrune)
## 
## Classification tree:
## snip.tree(tree = TrainTree, nodes = c(2L, 6L))
## Variables actually used in tree construction:
## [1] &quot;Sex&quot; &quot;Age&quot;
## Number of terminal nodes:  3 
## Residual mean deviance:  0.9857 = 699.9 / 710 
## Misclassification error rate: 0.1978 = 141 / 713
TrainPrune&lt;-prune.tree(TrainTree,best = 2,newdata=test,method = &quot;misclass&quot;)
plot(TrainPrune)
text(TrainPrune, cex=.9)</code></pre>
<p><img src="/projects/post/Titanic_Machine_Learning/index_files/figure-html/unnamed-chunk-9-3.png" width="672" /></p>
<pre class="r numberLines lineAnchors"><code>summary(TrainPrune)
## 
## Classification tree:
## snip.tree(tree = TrainTree, nodes = 2:3)
## Variables actually used in tree construction:
## [1] &quot;Sex&quot;
## Number of terminal nodes:  2 
## Residual mean deviance:  1.028 = 731.2 / 711 
## Misclassification error rate: 0.2104 = 150 / 713

PredSurv &lt;- predict(TrainTree, test, type=&quot;class&quot;)

CF&lt;-table(test$Survived,PredSurv)

Precision&lt;-CF[2,2]/(CF[2,1]+CF[2,2])
Precision
## [1] 0.6984127</code></pre>
